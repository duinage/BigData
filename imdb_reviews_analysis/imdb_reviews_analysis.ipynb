{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ad8889",
   "metadata": {},
   "source": [
    "# IMDB reviews exploration.\n",
    "## Author: Vadym Tunik.\n",
    "\n",
    "Dataset: Large Movie Review Dataset https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55188978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextual_search_system_for_related_texts as c\n",
    "from bow_model import BagOfWords, clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9e7a9",
   "metadata": {},
   "source": [
    "### System that offers five IMDB reviews that are similar to the given one.\n",
    "### (we intuitively believe that the user who wrote the review will be interested in finding a movie that evokes similar impressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data from: C:\\Users\\duina\\repo\\DA\\imdb_reviews_analysis\\aclImdb\\train\\unsup\n",
      "Attempting to load 50000 files (100.0% of total 50000 text files)...\n",
      "Successfully loaded 50000 out of 50000 attempted texts.\n",
      "\n",
      "Starting BoW process (use_bigrams=False)...\n",
      "Step 1: Cleaning and tokenizing texts...\n",
      "____ Processed 1000 texts...\n",
      "____ Processed 2000 texts...\n",
      "____ Processed 3000 texts...\n",
      "____ Processed 4000 texts...\n",
      "____ Processed 5000 texts...\n",
      "____ Processed 6000 texts...\n",
      "____ Processed 7000 texts...\n",
      "____ Processed 8000 texts...\n",
      "____ Processed 9000 texts...\n",
      "____ Processed 10000 texts...\n",
      "____ Processed 11000 texts...\n",
      "____ Processed 12000 texts...\n",
      "____ Processed 13000 texts...\n",
      "____ Processed 14000 texts...\n",
      "____ Processed 15000 texts...\n",
      "____ Processed 16000 texts...\n",
      "____ Processed 17000 texts...\n",
      "____ Processed 18000 texts...\n",
      "____ Processed 19000 texts...\n",
      "____ Processed 20000 texts...\n",
      "____ Processed 21000 texts...\n",
      "____ Processed 22000 texts...\n",
      "____ Processed 23000 texts...\n",
      "____ Processed 24000 texts...\n",
      "____ Processed 25000 texts...\n",
      "____ Processed 26000 texts...\n",
      "____ Processed 27000 texts...\n",
      "____ Processed 28000 texts...\n",
      "____ Processed 29000 texts...\n",
      "____ Processed 30000 texts...\n",
      "____ Processed 31000 texts...\n",
      "____ Processed 32000 texts...\n",
      "____ Processed 33000 texts...\n",
      "____ Processed 34000 texts...\n",
      "____ Processed 35000 texts...\n",
      "____ Processed 36000 texts...\n",
      "____ Processed 37000 texts...\n",
      "____ Processed 38000 texts...\n",
      "____ Processed 39000 texts...\n",
      "____ Processed 40000 texts...\n",
      "____ Processed 41000 texts...\n",
      "____ Processed 42000 texts...\n",
      "____ Processed 43000 texts...\n",
      "____ Processed 44000 texts...\n",
      "____ Processed 45000 texts...\n",
      "____ Processed 46000 texts...\n",
      "____ Processed 47000 texts...\n",
      "____ Processed 48000 texts...\n",
      "____ Processed 49000 texts...\n",
      "____ Initial token count (unique): 166113\n",
      "Step 2: Building final vocabulary...\n",
      "____ Postprocessing: Min Freq=5, Stopwords=Yes, Stemming=Yes\n",
      "____ Words after frequency filter: 42982\n",
      "____ Words after stopword filter: 42845\n",
      "____ Unique stemmed words: 27092\n",
      "____ Final vocabulary size: 27092\n",
      "Step 3: Creating BoW matrix...\n",
      "____ Vectorized 1000 texts...\n",
      "____ Vectorized 2000 texts...\n",
      "____ Vectorized 3000 texts...\n",
      "____ Vectorized 4000 texts...\n",
      "____ Vectorized 5000 texts...\n",
      "____ Vectorized 6000 texts...\n",
      "____ Vectorized 7000 texts...\n",
      "____ Vectorized 8000 texts...\n",
      "____ Vectorized 9000 texts...\n",
      "____ Vectorized 10000 texts...\n",
      "____ Vectorized 11000 texts...\n",
      "____ Vectorized 12000 texts...\n",
      "____ Vectorized 13000 texts...\n",
      "____ Vectorized 14000 texts...\n",
      "____ Vectorized 15000 texts...\n",
      "____ Vectorized 16000 texts...\n",
      "____ Vectorized 17000 texts...\n",
      "____ Vectorized 18000 texts...\n",
      "____ Vectorized 19000 texts...\n",
      "____ Vectorized 20000 texts...\n",
      "____ Vectorized 21000 texts...\n",
      "____ Vectorized 22000 texts...\n",
      "____ Vectorized 23000 texts...\n",
      "____ Vectorized 24000 texts...\n",
      "____ Vectorized 25000 texts...\n",
      "____ Vectorized 26000 texts...\n",
      "____ Vectorized 27000 texts...\n",
      "____ Vectorized 28000 texts...\n",
      "____ Vectorized 29000 texts...\n",
      "____ Vectorized 30000 texts...\n",
      "____ Vectorized 31000 texts...\n",
      "____ Vectorized 32000 texts...\n",
      "____ Vectorized 33000 texts...\n",
      "____ Vectorized 34000 texts...\n",
      "____ Vectorized 35000 texts...\n",
      "____ Vectorized 36000 texts...\n",
      "____ Vectorized 37000 texts...\n",
      "____ Vectorized 38000 texts...\n",
      "____ Vectorized 39000 texts...\n",
      "____ Vectorized 40000 texts...\n",
      "____ Vectorized 41000 texts...\n",
      "____ Vectorized 42000 texts...\n",
      "____ Vectorized 43000 texts...\n",
      "____ Vectorized 44000 texts...\n",
      "____ Vectorized 45000 texts...\n",
      "____ Vectorized 46000 texts...\n",
      "____ Vectorized 47000 texts...\n",
      "____ Vectorized 48000 texts...\n",
      "____ Vectorized 49000 texts...\n",
      "____ BoW matrix created. Shape: (50000, 27092)\n",
      "\n",
      "BoW process finished.\n",
      "____ Final Matrix Shape: (50000, 27092)\n",
      "____ Total time: 106.83 seconds\n",
      "\n",
      "Calculating similarity using 'cosine' distance...\n",
      "\n",
      "--- Similarity Results ---\n",
      "\n",
      "Chosen Text #2025 (Cleaned Snippet):\n",
      "how do we beginthe president is shot secret service agents chase down a shooter unconnected characters intersect in increasingly meaningless ways oh and did i mention the groundhog dayesqe time rewind audible audience laughter is not what you expect to hear in a thriller when a film with such a reasonable premise is butchered and ultimately ends up dying an ignoble death my first question was wher...\n",
      "\n",
      "Top 5 Most Similar Texts:\n",
      "\n",
      "Rank 1: Text #37426 with Distance: 0.42\n",
      "there used to be a time when star wars was the best thing since sliced bread of course i was about then but nonetheless it was the ultimate for many people of my generation and then we grew up but perhaps it is that i have grown up that i didnt enjoy this one wanting a film for adults i entered the cinema expecting some good old fashioned galatic action despite the let down of the phantom menace b...\n",
      "\n",
      "Rank 2: Text #17209 with Distance: 0.42\n",
      "i came away from dead mans shoes with a sense of pride a sense of acknowledgment a feeling that british film at its lowest budget level and at its grittiest and simplest is very much alive and active the reaction i had also affected the way i perceive hollywood a little bit more its true that films like zodiac blood diamond and apocalypto occupy my top three of but even some of them and the many o...\n",
      "\n",
      "Rank 3: Text #32196 with Distance: 0.43\n",
      "although im always careful to avoid others opinions before i see a film and make up my mind about it i often look at what others have to say after ive reached a conclusion im regularly flabbergasted by opinions on films such as sixteen tongueswhich i thought was horriblebecause they are often somewhat favorable it especially confounds me in light of the consistently negative reviews received by fi...\n",
      "\n",
      "Rank 4: Text #21974 with Distance: 0.43\n",
      "it is my opinion that isabelle huppert is the finest actress to ever step infront of a camera beautiful unusual talented beyond reason and able to convey more with a simple look than most actors can communicate in an entire film in short she is brilliant even when her films lag  she shines her interest in exploring the darker sides of humanity have created some of the most memorable performances p...\n",
      "\n",
      "Rank 5: Text #33615 with Distance: 0.43\n",
      "i liked training day i like christian bale together they should be a pretty great combination and together they were in david ayers directorial debut harsh times bale in particular as always got the notice of critics when the film hit the festival circuit and came and went in limited theatrical release late last year looking fairly interesting i gave the movie a watch and despite the praise ended ...\n"
     ]
    }
   ],
   "source": [
    "texts = c.load_texts_from_folder(c.FOLDER_PATH, fraction=c.DATA_FRACTION)\n",
    "bow_model = BagOfWords(use_bigrams=c.USE_BIGRAMS)\n",
    "bow_matrix = bow_model.fit_transform(texts, vocab_min_frequency=c.VOCAB_MIN_FREQUENCY)\n",
    "\n",
    "similar_indices, similar_distances = c.find_similar_texts(\n",
    "    bow_matrix,\n",
    "    chosen_index=c.CHOSEN_TEXT_INDEX,\n",
    "    num_similar=c.NUM_RELATED_TO_FIND,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Similarity Results ---\")\n",
    "chosen_text_original = texts[c.CHOSEN_TEXT_INDEX]\n",
    "cleaned_chosen_text = clean_text(chosen_text_original)\n",
    "print(f\"\\nChosen Text #{c.CHOSEN_TEXT_INDEX} (Cleaned Snippet):\")\n",
    "print(cleaned_chosen_text[:c.CHAR_LIMIT_FOR_TEXT] + ('...' if len(cleaned_chosen_text) > c.CHAR_LIMIT_FOR_TEXT else ''))\n",
    "\n",
    "print(f\"\\nTop {c.NUM_RELATED_TO_FIND} Most Similar Texts:\")\n",
    "for i in range(len(similar_indices)):\n",
    "    index = similar_indices[i]\n",
    "    distance = similar_distances[i]\n",
    "    related_text_original = texts[index]\n",
    "    cleaned_related_text = clean_text(related_text_original)\n",
    "\n",
    "    print(f\"\\nRank {i+1}: Text #{index} with Distance: {distance:.2f}\")\n",
    "    print(cleaned_related_text[:c.CHAR_LIMIT_FOR_TEXT] + ('...' if len(cleaned_related_text) > c.CHAR_LIMIT_FOR_TEXT else ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
